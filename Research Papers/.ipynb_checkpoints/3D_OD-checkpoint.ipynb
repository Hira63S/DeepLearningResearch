{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Object Detection Preparation for Tesla\n",
    "\n",
    "Vision-based approach\n",
    "no lidar, no HD map\n",
    "\n",
    "Bird-eye view networks.\n",
    "\n",
    "\n",
    "Neural Networks for FSD:\n",
    "Take an image, have it go through the shared backbone and make predictions about the image in the pixel space. \n",
    "\n",
    "For example: Taking edges, take predictions of road edge task from all 8 cameras.\n",
    "can't drive on the raw predictions of the edges in 2D pixel space. Project it out to a bird's eye view.\n",
    "Project them out into 3D and do an occupency tracker.\n",
    "Stitch them across camera and across time.\n",
    "This is where Software 2.0 comes up. software 1.0 code, \n",
    "Take the functionality from 1.0 code base and put it into software 2.0 code.\n",
    "In the case of road edges, \n",
    "bird's eye view networks -> Take all the camera, feed through backbone, have neural net fusion layer, that stitches up the feature maps across the different views and does the prediction from image space to bird's eye view and temporal module that smoothes out the predictions. dead net declutter that does a top-down approach\n",
    "\n",
    "Depth-per pixel to make the prediction.\n",
    "Bird's eye view networks. \n",
    "\n",
    "Depth predictions -- pseudolidar approach\n",
    "predict the depth per pixel and cast it out\n",
    "self-supervised techniques \n",
    "\n",
    "## Achieving depth from image\n",
    "self-superised tech -> predict a depth on the image, cast the pixels in 3D. and reproject them into camera views at the same time but different camera, or you can project them into future frames. \n",
    "Photometric loss. \n",
    "To make everything consistent is to predict the correct depth. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Approaches:\n",
    "\n",
    "Pseudo LIDAR approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird-eye view networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3D Convolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Used to perform convolutions in CNNs to compute both the spatial and temporal dimensions. \n",
    "- 3D convolution convolves the 3D filters to the cube performed by stacking the subsequent frames on top of each other so we are able to connect contiguous frame together. \n",
    "\n",
    "~ optical flow\n",
    "~ computing gradients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Paper: End-to-end learning of deep convolutional neural network for 3D human action recognition\n",
    "\n",
    "Uses 3D filters that perform convolutions on 3 subsequent frames so that we can follow the objects through the video i.e. tag an object and be able to recognize it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4D convolutions for Spatial-Temporal Processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stereo Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does not rely on LIDAR data as input or as supervision during training, but instead takes RGB images with corresponding annotated 3D bounding boxes as training data. First step is to extract the objects from the background via stereo Region Proposal Networks (RPNs) so that the background does not interfere with 3D object Detection.\n",
    "Instance-Depth-Aware(IDA) module predicts the center depth of an object's 3D bounding box. Instead of focusing on correspondence of each pixel between images, the module focuses on correspondance of each instance. It predicts the depth of the 3D bounding box's center by instance-depth awareness, disparity adaptation and matching cost reweighting.\n",
    "\n",
    "## Method:\n",
    "Training data: RGB images with corresponding annotation boxes. \n",
    "Training: END-TO-END mapping from an image pair to object 3D bounding boxes.\n",
    "3D OD errors stem from an error in depth estimation z of a 3D bounding box center so a separate regression model is used to obtain instance depth.\n",
    "\n",
    "# Object Instance Detection\n",
    "Stereo-Images:\n",
    "we extract a pair of RoI for each object in the left and right images from the stereo RPN module. This helps in avoiding the complex matching of all pixels between the left and right images and eliminate the adverse effect of background on object detection.\n",
    "The RPN creates a union RoI for each object whose size and location is the same on both left and right images so that we can ensure the starting points of each pair of RoIs.\n",
    "RoIAlign is applied on both left and right feature maps and then, they are concatenated and fed into stereo regression network to predict position, orientation and dimensions of 3D bounding box respectively. \n",
    "IDA module is used to detect the object center to account for the errors in 3D OD.\n",
    "\n",
    "## Instance Disparity Estimation:\n",
    "Instead of focusing on regressing the disparity of each pixel between the stereo images, we are interested in computing the disparity of each instance to locate its position. Focusing more on global spatial information of the object.\n",
    "After forming a cost volume of dimensionality disparity x height x width x feature size by concatenating the left and right feature maps across each disparity level, the feature maps are then passed into two consecutive 3D convolution layers, each following by a 3D max-pooling layer. The max-pooling layer learns and performs down sampling on feature representations from the cost volume. To obtain the depth representation of the object, we transform the disparity because disparity is inversely proportional to depth. \n",
    "\n",
    "We get the depth probability of the 3D box center by performing the sum of each depth z weighted by its normalized probability.\n",
    "\n",
    "## Training:\n",
    "Model is trained with supervised learning where we find the regression loss as the error between the ground truth depth and the model's predicted depth. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the following array of values, print out all the elements in reverse order, with each element on a new line.\n",
    "For example, given the list\n",
    "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n",
    "Your output should be\n",
    "0\n",
    "1\n",
    "2\n",
    "3\n",
    "4\n",
    "5\n",
    "6\n",
    "7\n",
    "8\n",
    "9\n",
    "10\n",
    "You may use whatever programming language you'd like.\n",
    "Verbalize your thought process as much as possible before writing any code. Run through the UPER problem solving framework while going through your thought process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region Proposal Network\n",
    "RPN for extracting Regions of Interest from each object in the left and right images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
