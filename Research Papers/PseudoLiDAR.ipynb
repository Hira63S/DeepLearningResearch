{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-LIDAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "An approach to creating LIDAR-like point clouds without working with stereo images, or depth camera but just working with a single image input (is applicable to Tesla).\n",
    "\n",
    "## Important Pipeline:\n",
    "\n",
    "1. Get the image.\n",
    "2. Predict the depth of each pixel.\n",
    "3. Cast out the pixels. (simulating LiDAR)\n",
    "4. Get the predictions and project them onto the cameras\n",
    "- Self-supervised techniques -> Predict a depth.\n",
    "- Once you do have an 3D image, you cast out the pixels.\n",
    "- Re-project them into different cameras or in the future frames of the camera.\n",
    "- And you will have the photometric loss.\n",
    "\n",
    "\n",
    "## First Step:\n",
    "- Perform monocular depth estimation and generate pseudo-LiDAR for the entire scene by **lifting every pixel** within the image into its 3D coordinate.\n",
    "- Then train LiDAR-based 3D detection network with the pseudo-LiDAR.\n",
    "- Using LiDAR-based 3D detector, **Frustum PointNets**, we detect the 2D object proposals in the input image and extract a **point cloud frustum** from the pseudo-LIDAR for each 2D proposal. Then, an oriented 3D bounding box is detected for each frustum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems:\n",
    "1. Depth estimation based on a monocular image is inaccurate because of local misalignment, especially for the objects that are far off.\n",
    "2. The extracted Point cloud always has a long-tail because it is hard to estimate depth near the edge/periphery of an object. This means that there are always extra points that are shown as belonging to the object when they actually don't."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solutions:\n",
    "1. To solve local misalignment, when projecting the 3D box onto the image, we use a 2D-3D bounding box consistency constraint i.e. the 3D bounding box overlaps with the 2D detected proposals on the image. During training, we formulate the constraint as bounding box consistency loss (BBCL) to supervise learning.\n",
    "    - During testing, a bounding box consistency optimization (BBCO) is solved subject to this constraint using a global optimization method to further improve the prediction results.\n",
    "\n",
    "2. To deal with the long-tail of points proposed as belonging to the object, we porpose to use mask segmentation instead of 2D bounding boxes around the object because that would define the object pixel by pixel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other Approaches\n",
    "Models using 2D-3D bounding box consistency constraint are also used to predict 3D bounding boxes using 2D processing. For example, one proposal is to use 2D CNNs to predict a subset of features like the object orientation and size. During testing, we combine the estimates with the constraint to compute the remaining parameters like the object center location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pseudo-LiDAR Approach:\n",
    "\n",
    "Goal: Using one RGB image to estimate 3D bounding box of objects.\n",
    "Parameters for the 3D bounding box (total 7):\n",
    "Object center: (x,y,z)\n",
    "Object's size: (h, w, l)\n",
    "Heading angle: (theta)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Pseudo LiDAR](PseudoLiDAR.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach\n",
    "Input image is passed into two modules simultaneously:\n",
    "a. Pseudo-LiDAR Generator\n",
    "b. 2D Instance Mask Proposal detection (proposal loss is used to train this part of net)\n",
    "\n",
    "The outputs from both are put together into Frustum PointNet which does 3D point cloud segmentation -> Using 3D segmentation Loss, we optimize the point cloud, then we pass it into 3D box estimation module and 3D box correction module simultaneously. \n",
    "1. 3D box estimation module outputs the 7 parameters which are added with the 7 parameters output by the correction module and then, we pass on the final estimate. We then project it onto the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monocular Depth Estimation:\n",
    "\n",
    "DORN network comes with pre-trained weights that serves the purpose of estimating monocular depth using a single RGB image. We do not update the weights of the network and so it can be thought of as an offline module. \n",
    "\n",
    "# Pseudo-LiDAR Generation:\n",
    "After we have gotten the depth from the DORN model, we can use the depth estimate and the camera matrix to calculate the object's 3D location. When we have the camera matrix, we can calculate the 3D location of the object in the image. We can also project that location onto the world because we have the camera extrinsic matrix C = [Rt]\n",
    "\n",
    "## LiDAR Vs. Pseudo-LiDAR\n",
    "- The point clouds made by pseudo-LiDAR approach tend to have a long-tail i.e. more point clouds because it is hard to estimate depth around the edge of the object.\n",
    "- For the far away object, the extracted point cloud frustum might be largely off and there is a local misalignment with respect to the LiDAR point cloud.\n",
    "\n",
    "Another  factor contributing to the difference between the two approaches is that the LiDAR tends to have low density of the point cloud whereas the Pseudo-LiDAR approach tends to have high density."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instance Mask Proposal Detection\n",
    "\n",
    "One of the ways to deal with the long-tail issues is that we use mask for the object instance. That way, we only keep the points in the 3D point cloud that overlap with the mask pixels and ignore the rest. \n",
    "\n",
    "# Modeling:\n",
    "\n",
    "After getting the point cloud and then, the 2D mask, we can extract a set of point cloud frustums, that can be passed onto training a two stage LiDAR-based 3D detection algorithm for 3D bounding box prediction. The paper used Frustum PointNets (to be looked at in detail). But essentially, using the point clouds extracted from the first couple of techniques, we sample a fixed number of point clouds after segmenting the point cloud frustum, and use those small number of points to estimate the center, size and heading angle. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 2D-3D Bounding Box Consistency Optimization (BBCO)\n",
    "\n",
    "To refine the bounding box estimate, we use geometry to fix the issue and we try to look at whether the 2D box also did not have a good alignment if there is a local alignment issue in the output of the model.\n",
    "To compare the two, we first convert the 7 points i.e. x,y,z,height,width,length and theta into 8 points for the box's corners. Then, we convert those into 2D project. Then, we calculate the minimum bounding rectangle that represents the smallest axis-aligned 2D bounding box that can enclose the 2D point set. We also get the 2d representation of the mask this way.\n",
    "\n",
    "## Bounding Box Consistency Loss: \n",
    "Box correction module: it takes in the segmented point cloud and features extracted from the 3D box estimation module as the input, and outputs a correction of the 3D bounding box parameters (i.e. residuals). Then, the final estimate can be computed as the summation over the initial estimate and the residual. Since this approach is differentiable, the model can be trained end-to-end with BBCL. \n",
    "\n",
    "### Post Processing:\n",
    "Bounding Box Consistency Optimization:\n",
    "Using global search optimization method, we can refine the final estimate with the BBC constraing as a post-processing step.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PointNets\n",
    "Takes in point clouds as input and outputs either class labels for the entire input or per point segment/part labels for each point of the input. Each point is processed independently, where it is represented by just three coordinates.\n",
    "\n",
    "## Use of Max Pooling\n",
    "For each point, we use max pooling to choose the most important points. \n",
    "The final FC layers use the max pooled point clouds to perform the classification and segmentation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
