{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monocular Plan View Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "The RGB image is taken through a CovNet to make 2D object detection predictions.\n",
    "The coordinates for the objects are then passed through a 3D network that compute the depth, the rotation and 3D size of each object.\n",
    "A reprojection layer puts each 3D object into plan view image using the 3D bounding box info.\n",
    "The originalimage and the generated plan view are passed through separate convnets and globally pooled into two feature vectors.\n",
    "The resulting vectors are concatenated and fed to a fully conv layer which outputs discrete action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird's Eye View Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network stack is taking more and more software 1.0. No Heuristics, only if the network says so.\n",
    "Stitch up of the layout. Stitching across time and across space of the output from all the camera outputs.\n",
    "Individual views go through the camera, there is a fusion layer that extracts features. Fusion layer that does things like feature transforms and re-puts it in the image space and temporally smooth it. Decoder gives all the predictions. Bird's eye view networks: Take in the views from all the images and processes it to create a layout of the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Psudo-LiDAR approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between LiDAR proposals and image generated 3D proposals is because of how the image generated proposals are displayed. Most of the times the 3D representations are shown as additional channels and that makes it really hard for the model to localize and detect small objects. Also the far away or small objects in the image get grouped together the points from far-away regions of 3D space. Making it even harder ot detect and localize objects in 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome these challenges, there is a two-step approach for stereo-based 3D object detection proposed. \n",
    "1. First, there is the conversion of estimated depth map form stereo or monocular imagery into a 3D point clouds which are then used to train the 3D OD models built for LiDAR data.\n",
    "2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
