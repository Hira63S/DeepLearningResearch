{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monocular Plan View Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture\n",
    "\n",
    "The RGB image is taken through a CovNet to make 2D object detection predictions.\n",
    "The coordinates for the objects are then passed through a 3D network that compute the depth, the rotation and 3D size of each object.\n",
    "A reprojection layer puts each 3D object into plan view image using the 3D bounding box info.\n",
    "The originalimage and the generated plan view are passed through separate convnets and globally pooled into two feature vectors.\n",
    "The resulting vectors are concatenated and fed to a fully conv layer which outputs discrete action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bird's Eye View Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network stack is taking more and more software 1.0. No Heuristics, only if the network says so.\n",
    "Stitch up of the layout. Stitching across time and across space of the output from all the camera outputs.\n",
    "Individual views go through the camera, there is a fusion layer that extracts features. Fusion layer that does things like feature transforms and re-puts it in the image space and temporally smooth it. Decoder gives all the predictions. Bird's eye view networks: Take in the views from all the images and processes it to create a layout of the world. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Psudo-LiDAR approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between LiDAR proposals and image generated 3D proposals is because of how the image generated proposals are displayed. Most of the times the 3D representations are shown as additional channels and that makes it really hard for the model to localize and detect small objects. Also the far away or small objects in the image get grouped together the points from far-away regions of 3D space. Making it even harder ot detect and localize objects in 3D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To overcome these challenges, there is a two-step approach for stereo-based 3D object detection proposed. \n",
    "1. First, there is the conversion of estimated depth map form stereo or monocular imagery into a 3D point clouds.\n",
    "2. Then, the psuedo-LiDAR depth maps produced from stereo or monocular imagery are used to train the 3D OD models built for LiDAR data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**DORN**](https://arxiv.org/abs/1806.02446) and [**Siamese networks**] compute multi-scale features with ordinal regression to predict pixel depth with remarkably low errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approach:\n",
    "\n",
    "To close the gap between LiDAR and camera-based 3D depth maps, we are simply focused on represeting the data in a different format. Currently, the data from monocular or stereo images is represented as additional channels. But just changing the representation of the data would help in increasing the accuracy of 3D detection with just cameras.\n",
    "\n",
    "#### Plan:\n",
    "Take the depth maps from monocular or stereo imagery and then, back-project the pixels into a 3D poitn cloud. Since we are creating a pseudo-LiDAR signal, we can use any LiDAR based models already built.\n",
    "\n",
    "### Depth Estimation:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Convs comparison with Pseudo-LiDAR:\n",
    "\n",
    "2D Convs make two assumptions about the way they work:\n",
    "1. Local neighborhoods in images have meaning and the network needs to look at the local matches to make accurate predictions.\n",
    "2. All neighborhoods can be operated upon in an identical manner.\n",
    "\n",
    "### Solutions:\n",
    "- Local patches in 2D images are only coherent physically if they are entirely contains i na single object. \n",
    "- objects that occur at multiple depths project to *different scales* in the depth map.\n",
    "\n",
    "In contrast to 2D detection, the 3D convs on point couds or 2D convs on bird's eye view slices operate on pixels that are physically close together, building on the logic that the pixels that show up at roughly the different height but a particular spatial location in the point cloud do belong to the same object. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentation:\n",
    "\n",
    "Details of the Approach:\n",
    "\n",
    "**PSMNet** for *dense disparity.*\n",
    "**DORN** for *monocular depth estimation.*\n",
    "**Pseudo-LiDAR generation** - back-projecting the estimated depth map into 3D points in the Velodyne LiDAR's coordinate system, using the calibration matrics.\n",
    "**3D Object Detection** Using PointNets and AVOD-FPN. Initially both are trained with LiDAR and monocular images, but the paper's researchers trained them with psuedo-LiDAR data generated from stereo disparity estimation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words to look for:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Disparity estimation](https://arxiv.org/abs/1810.11408?)\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
