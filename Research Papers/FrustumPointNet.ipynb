{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frustum PointNets PyTorch Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline\n",
    "1. Backbone i.e. ResNet\n",
    "2. FPN - Object Detection\n",
    "3. Taking the Object Detection proposal in the 2D image and change them to 3D frustums.\n",
    "4. Train the model for 3D frustums\n",
    "- More details to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "import matplotlib.pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Do not need a basic block but that just consists of:\n",
    "- conv layer\n",
    "- batch norm\n",
    "- relu\n",
    "- conv layer_2\n",
    "- batch norm_2\n",
    "- Downsample i.e. shortcut\n",
    "- add the final output to batch_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helping functions:\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\" 3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride, groups=groups, \\\n",
    "                    dilation=dilation, padding=dilation, bias=False)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\" 1x1 convoltion with padding \"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "    \"\"\" Creation of basic block of resnet that use bottleneck method of\n",
    "    having 1x1, 3x3, & 1x1convs\"\"\"\n",
    "    expansion = 4\n",
    "    def __init__(self, in_planes, out_planes, stride=1, downsample=None, groups=1, base_width=64,\n",
    "                 dilation=1, norm_layer=None):\n",
    "        \n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "            \n",
    "        width = int(out_planes * (base_width / 64.)) * groups\n",
    "        # when stride != 1, both self.conv2 and self.downsample layers downsample the input\n",
    "        \n",
    "        self.conv1 = conv1x1(in_planes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, out_planes*self.expansion)\n",
    "        self.bn3 = norm_layer(out_planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "        \n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False, groups=1, width_per_group=64,\n",
    "                replace_stride_with_dilation=None, norm_layer=None):\n",
    "        \n",
    "        super(ResNet, self).__init__()\n",
    "        \n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        \n",
    "        self._norm_layer = norm_layer\n",
    "        \n",
    "        self.in_planes = 64\n",
    "        self.dilation =1\n",
    "        \n",
    "        if replace_stride_with_dilation is None:\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        \n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride should be none\")\n",
    "            \n",
    "        \n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, self.in_planes, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = norm_layer(self.in_planes)\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2, \\\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2, \\\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3], stride=2, \\\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        \n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "                                       \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                                       \n",
    "    \n",
    "    def _make_layer(self, block, out_planes, blocks, stride=1, dilate=False):\n",
    "        \n",
    "        norm_layer = self._norm_layer\n",
    "        downsample=None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride=1\n",
    "        \n",
    "        if stride != 1 or self.in_planes != out_planes * block.expansion:\n",
    "            downsmaple = nn.Sequential(\n",
    "                        conv1x1(self.in_planes, out_planes * block.expansion, stride),\n",
    "                        norm_layer(out_planes * block.expansion),\n",
    "            )\n",
    "            \n",
    "        \n",
    "        layers= []\n",
    "        layers.append(block(self.in_planes, out_planes, stride, downsample, self.groups,\n",
    "                            self.base_width, previous_dilation, norm_layer))\n",
    "        \n",
    "        self.in_planes = out_planes * block.expansion\n",
    "        \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.in_planes, out_planes, groups=self.groups, \\\n",
    "                                base_width=self.base_width, dilation = self.dilation,\n",
    "                                norm_layer = norm_layer))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    \n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "def resnet101(pretrained=False, progress=True, **kwargs):\n",
    "    r\"\"\"ResNet-101 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet101', Bottleneck, [3, 4, 23, 3], pretrained, progress,\n",
    "                   **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet101()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_node(in_planes, out_planes):\n",
    "    \"\"\"\n",
    "    2 conv layers - first layer does 256, 256 and second does 256 -> 128\n",
    "    \n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, in_planes, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "def smooth(in_planes, out_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n",
    "\n",
    "def predict(in_planes, out_planes):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=1, padding=1),\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "\n",
    "def upshuffle(in_planes, out_planes, upscale_factor):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_planes, out_planes*upscale_factor**2, kernel_size=3, stride=1, padding=1),\n",
    "        nn.PixelShuffle(upscale_factor),\n",
    "        nn.ReLU(),\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Pyramid Networks:\n",
    "To use the **ConvNet** Pyramidal feature heirarchy, we use the semantics from low to high levels and build the feature pyramid network.\n",
    "\n",
    "Bottom-up: The main ConvNet backbone that produces different feature maps at several scales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class I2D(nn.Module):\n",
    "    def __init__(self, pretrained=True, fixed_feature_weights=False):\n",
    "        super(I2D, self).__init__()\n",
    "        \n",
    "        resnet = resnet101(pretrained=pretrained)\n",
    "        \n",
    "        if fixed_feature_weights:\n",
    "            for p in resnet.parameters():\n",
    "                p.requires_grad = False\n",
    "        \n",
    "        self.layer0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool)\n",
    "        self.layer1 = nn.Sequential(resnet.layer1)\n",
    "        self.layer2 = nn.Sequential(resnet.layer2)\n",
    "        self.layer3 = nn.Sequential(resnet.layer3)\n",
    "        self.layer4 = nn.Sequential(resnet.layer4)\n",
    "        \n",
    "        # top layer\n",
    "        self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # lateral layers\n",
    "        self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer2 = nn.Conv2d(512, 256, kernel_size=1, stride=1, padding=0)\n",
    "        self.latlayer3 = nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "        # smooth layers\n",
    "        self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Aggregate layers\n",
    "        self.agg1 = agg_node(256, 128)\n",
    "        self.agg2 = agg_node(256, 128)\n",
    "        self.agg3 = agg_node(256, 128)\n",
    "        self.agg4 = agg_node(256, 128)\n",
    "        \n",
    "        # Upshuffle Layers\n",
    "        self.up1 = upshuffle(128, 128, 8)\n",
    "        self.up2 = upshuffle(128, 128, 4)\n",
    "        self.up3 = upshuffle(128, 128, 2)\n",
    "        \n",
    "        # depth prediction\n",
    "        self.predict1 = smooth(512, 128)\n",
    "        self.predict2 = predict(128, 1)\n",
    "        \n",
    "        \n",
    "    def _upsample_add(self, x, y):\n",
    "        '''Upsample and add two feature maps.\n",
    "        Args:\n",
    "          x: (Variable) top feature map to be upsampled.\n",
    "          y: (Variable) lateral feature map.\n",
    "        Returns:\n",
    "          (Variable) added feature map.\n",
    "        Note in PyTorch, when input size is odd, the upsampled feature map\n",
    "        with `F.upsample(..., scale_factor=2, mode='nearest')`\n",
    "        maybe not equal to the lateral feature map size.\n",
    "        e.g.\n",
    "        original input size: [N,_,15,15] ->\n",
    "        conv2d feature map size: [N,_,8,8] ->\n",
    "        upsampled feature map size: [N,_,16,16]\n",
    "        So we choose bilinear upsample which supports arbitrary output sizes.\n",
    "        '''\n",
    "        _,_,H,W = y.size()\n",
    "        return F.upsample(x, size=(H,W), mode='bilinear') + y\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _,_,H,W = x.size()\n",
    "        \n",
    "        # bottom-up\n",
    "        c1 = self.layer0(x)\n",
    "        c2 = self.layer1(c1)\n",
    "        c3 = self.layer2(c2)\n",
    "        c4 = self.layer3(c3)\n",
    "        c5 = self.layer4(c4)\n",
    "        \n",
    "        # top_down\n",
    "        p5 = self.toplayer(c5)\n",
    "        p4 = self._upsample_add(p5, self.latlayer1(c4))\n",
    "        p4 = self.smooth1(p4)\n",
    "        p3 = self._upsample_add(p4, self.latlayer2(c3))\n",
    "        p3 = self.smooth2(p3)\n",
    "        p2 = self._upsample_add(p3, self.latlayer3(c2))\n",
    "        p2 = self.smooth3(p2)\n",
    "        \n",
    "        d5, d4, d3, d2 = self.up1(self.agg1(p5)), self.up2(self.agg2(p4)), self.up3(self.agg4(p3)), self.agg4(p2)\n",
    "        _, _, H, W = d2.size()\n",
    "        vol = torch.cat( [F.upsample(d, size=(H,W), mode='bilinear') for d in [d5, d4, d3, d2] ], dim=1)\n",
    "        \n",
    "        return self.predict2( self.predict1(vol) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Frustum Proposal Generation\n",
    "-> Take image data,\n",
    "-> Get the object detected\n",
    "-> create a frustum from that 2D box i.e. convert it into 3D point clouds.\n",
    "-> Normalize the furstums by rotating them toward a center view such that the center axis of the frustum is orthogonal to the image plane. \n",
    "\n",
    "Three total nets involved:\n",
    "- 3D Instance Segmentation PointNet\n",
    "- T-Net \n",
    "- Amodal box estimation PointNet\n",
    "Loss functions are used for multi-task losses:\n",
    "L_c1-reg is for T-Net\n",
    "L_c2-reg is for center regression of box estimation net.\n",
    "L_h-cls and L_h-reg are losses for heading angle predictiom\n",
    "L_s-cls and L_s-reg are for box size regression.\n",
    "Softmax is used for all classification tasks and smooth-l1 is used for all regression cases.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "\n",
    "Pre-train the model weights on ImageNet classification and COCO object detection -> Fine-tune on KITTI dataset to classify and predict amodal 2D boxes.\n",
    "Note: Segmentation is done in 3D point clouds instead of 2D images.\n",
    "Each point in the point cloud gets classified. Based on the orientation of the frustum, it might get classified as one object but might become a cluttered or occluded point in another. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    def __init__(self, in_planes, out_planes, stride=1, shortcut=None, groups=1, dilation=1,\n",
    "                 base_width=64, norm_layer=None):\n",
    "        \n",
    "        super(ActivationBlock, self).__init__()\n",
    "        \n",
    "        # check for exception\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 64:\n",
    "            raise ValueError('PreActivationBlock only supports groups = 1 and base_width =64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError('Not supported')\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels=in_planes, out_channels=out_planes, groups=groups, \\\n",
    "                                kernel_size=3, stride=stride, padding=1, \\\n",
    "                                bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(in_channels=in_planes, out_channels=out_planes, \\\n",
    "                                kernel_size=3, stride=stride, padding=1, \\\n",
    "                                bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(in_planes)\n",
    "        \n",
    "        if stride != 1 or in_planes != self.expansion * out_planes:\n",
    "            # mainly done so that the output channels of the shortcut layer are \n",
    "            # the same as the conv_2 layer output channels\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels=in_planes, \\\n",
    "                          out_channels= self.expansion*out_planes,\n",
    "                          kernel_size = 1,\n",
    "                          stride=stride,\n",
    "                          bias = False))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.Relu(self.bn_1(x))\n",
    "        # we want the output from the first bach_norm layer to be the shortcut\n",
    "        shortcut = self.shortcut(x) if hasattr(self, 'shortcut') else x\n",
    "        \n",
    "        x = self.conv_1(x)\n",
    "        x = F.Relu(self.bn_2(x))\n",
    "        x = self.conv_2(x)\n",
    "        x += shortcut\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreActivationNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        \n",
    "        super(PreActivationNet, self).__init__()\n",
    "        \n",
    "        # these in_planes are passed into the make_group function\n",
    "        \n",
    "        self.in_planes = 64\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias = False)\n",
    "        \n",
    "        self.layer_1 = self._make_group(block, 64, num_blocks[0], stride=1)\n",
    "        self.layer_2 = self._make_group(block, 128, num_blocks[1], stride=2)\n",
    "        self.layer_3 = self._make_group(block, 256, num_blocks[2], stride=2)\n",
    "        self.layer_4 = self._make_group(block, 512, num_blocks[3], stride=2)\n",
    "        \n",
    "        self.linear = nn.Linear(512 * block.expansion, num_classes)\n",
    "        \n",
    "    \n",
    "    def _make_group(self, block, out_planes, num_blocks, stride):\n",
    "        \"\"\"\n",
    "        create one residual group\n",
    "        \"\"\"\n",
    "        strides = [stride] + [1] * (num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, out_planes, stride))\n",
    "            self.in_slices = out_planes * block.expansion\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.conv_1(x)\n",
    "        out = self.layer_1(out)\n",
    "        out = self.layer_2(out)\n",
    "        out = self.layer_3(out)\n",
    "        out = self.layer_4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.linear(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resnet34():\n",
    "    return PreActivationNet(block=PreActivationBlock, num_blocks = [3, 4, 6, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resnet34()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet101\n",
    "\n",
    "- conv1 layer -> bn1 layer -> ReLu, inplace = True -> layer1\n",
    "- Layer_1: conv1 -> bn1 ----> bn3 -> relu -> downsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet101()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
